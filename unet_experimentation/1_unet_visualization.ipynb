{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Visualization\n",
    "\n",
    "In this notebook, we'll be experimenting with the UNet, trying to visualize what happens at each layer. I'll be hijacking the MONAI UNet architecture to output some nice images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelled after the original UNet\n",
    "class UNet(nn.Module):\n",
    "    # This conv/relu combination results in no change in dimension for full image restoration\n",
    "    def conv_relu(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='reflect'):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                padding_mode=padding_mode\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    # This transpose doubles the dimensions\n",
    "    def conv_transpose(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1):\n",
    "        return nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            output_padding=output_padding\n",
    "        )\n",
    "    \n",
    "    def first_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            self.conv_relu(in_channels, out_channels),\n",
    "            self.conv_relu(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    # Output: (x-4)/2\n",
    "    def contract_block(self, in_channels, out_channels):\n",
    "        # Testing: adding BatchNorm2d(out_channels) after ReLU layers\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            self.conv_relu(in_channels, out_channels),\n",
    "            self.conv_relu(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def bottleneck_block(self, in_channels, mid_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            self.conv_relu(in_channels, mid_channels),\n",
    "            self.conv_relu(mid_channels, mid_channels),\n",
    "            self.conv_transpose(mid_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    # Output: (x-4)*2\n",
    "    def expand_block(self, in_channels, mid_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            self.conv_relu(in_channels, mid_channels),\n",
    "            self.conv_relu(mid_channels, mid_channels),\n",
    "            self.conv_transpose(mid_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def final_block(self, in_channels, mid_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            self.conv_relu(in_channels, mid_channels),\n",
    "            self.conv_relu(mid_channels, mid_channels),\n",
    "            nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.contraction = nn.ModuleList([\n",
    "            # 288\n",
    "            self.first_block(1, 64),\n",
    "            # 288\n",
    "            self.contract_block(64, 128),\n",
    "            # 144\n",
    "            self.contract_block(128, 256),\n",
    "            # 72\n",
    "            self.contract_block(256, 512),\n",
    "            # 36\n",
    "        ])\n",
    "        \n",
    "        self.bottleneck = self.bottleneck_block(512, 1024, 512)\n",
    "        \n",
    "        self.expansion = nn.ModuleList([\n",
    "            # 36\n",
    "            self.expand_block(1024, 512, 256),\n",
    "            # 72\n",
    "            self.expand_block(512, 256, 128),\n",
    "            # 144\n",
    "            self.expand_block(256, 128, 64),\n",
    "            # 288\n",
    "            self.final_block(128, 64, 2)\n",
    "            # 288\n",
    "        ])\n",
    "        \n",
    "        self.contraction_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        for layer in self.contraction:\n",
    "            image = layer(image)\n",
    "            self.contraction_outputs.append(image)\n",
    "        \n",
    "        image = self.bottleneck(image)\n",
    "        for i in range(4):\n",
    "            image = torch.cat((self.contraction_outputs[3 - i], image), dim=1)\n",
    "            image = self.expansion[i](image)\n",
    "        self.contraction_outputs = []\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
