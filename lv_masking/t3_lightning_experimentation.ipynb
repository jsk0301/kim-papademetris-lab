{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models\n",
    "\n",
    "This notebook will contain all of the different variations of the 2D UNet I want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Code\n",
    "The code in this section needs to be run for every test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.transforms import \\\n",
    "    Compose, LoadNiftid, ScaleIntensityd, AddChanneld, ToTensord, RandSpatialCropd\n",
    "from monai.data import list_data_collate\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "# These imports are to recreate MONAI's UNet\n",
    "from monai.networks.blocks.convolutions import Convolution, ResidualUnit\n",
    "from monai.networks.layers.factories import Norm, Act\n",
    "from monai.networks.layers.simplelayers import SkipConnection\n",
    "from monai.utils import export\n",
    "from monai.utils.aliases import alias\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Data Collection\n",
    "data_dir = 'data/'\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "images = sorted(glob.glob(data_dir + '**/*CTImg*', recursive=True))\n",
    "masks = sorted(glob.glob(data_dir + '**/*Mask*', recursive=True))\n",
    "data_dicts = [{'image': image_file, 'mask': mask_file} for image_file, mask_file in zip(images, masks)]\n",
    "\n",
    "def select_animals(images, masks, animals):\n",
    "    \"\"\"Returns the images and masks as a dictionary for specific animals.\"\"\"\n",
    "    filtered_images = []\n",
    "    filtered_masks = []\n",
    "    for animal in animals:\n",
    "        filtered_images.extend(filter(lambda x: 'PSEA' + str(animal) in x, images))\n",
    "        filtered_masks.extend(filter(lambda x: 'PSEA' + str(animal) in x, masks))\n",
    "    return [{'image': image_file, 'mask': mask_file} for image_file, mask_file in zip(filtered_images, filtered_masks)]\n",
    "\n",
    "# Note: I want to keep the datasets for train/val/test constant across my different network for some consistency\n",
    "train_dicts = select_animals(images, masks, [12, 13, 14, 18, 20])\n",
    "val_dicts = select_animals(images, masks, [25])\n",
    "test_dicts = select_animals(images, masks, [27])\n",
    "data_keys = ['image', 'mask']\n",
    "\n",
    "# Creating data loaders from data dicts\n",
    "def create_loader(data_dicts, transforms=None, batch_size=1, shuffle=False):\n",
    "    dataset = monai.data.CacheDataset(\n",
    "        data=data_dicts,\n",
    "        transform=transforms,\n",
    "        cache_rate=1.0\n",
    "    )\n",
    "    data_loader=DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "I used MONAI's UNet source code inside PyTorch Lightning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(LightningModule):\n",
    "    \n",
    "    # MONAI UNet code begins here\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        channels,\n",
    "        strides,\n",
    "        criterion,\n",
    "        kernel_size=3,\n",
    "        up_kernel_size=3,\n",
    "        num_res_units=0,\n",
    "        act=Act.PRELU,\n",
    "        norm=Norm.INSTANCE,\n",
    "        dropout=0,\n",
    "        learning_rate=0.001,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(channels) == (len(strides) + 1)\n",
    "        self.dimensions = dimensions\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.criterion = criterion\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_kernel_size = up_kernel_size\n",
    "        self.num_res_units = num_res_units\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate=0.001\n",
    "\n",
    "        def _create_block(inc, outc, channels, strides, is_top):\n",
    "            \"\"\"\n",
    "            Builds the UNet structure from the bottom up by recursing down to the bottom block, then creating sequential\n",
    "            blocks containing the downsample path, a skip connection around the previous block, and the upsample path.\n",
    "            \"\"\"\n",
    "            c = channels[0]\n",
    "            s = strides[0]\n",
    "\n",
    "            if len(channels) > 2:\n",
    "                subblock = _create_block(c, c, channels[1:], strides[1:], False)  # continue recursion down\n",
    "                upc = c * 2\n",
    "            else:\n",
    "                # the next layer is the bottom so stop recursion, create the bottom layer as the sublock for this layer\n",
    "                subblock = self._get_bottom_layer(c, channels[1])\n",
    "                upc = c + channels[1]\n",
    "\n",
    "            down = self._get_down_layer(inc, c, s, is_top)  # create layer in downsampling path\n",
    "            up = self._get_up_layer(upc, outc, s, is_top)  # create layer in upsampling path\n",
    "\n",
    "            return nn.Sequential(down, SkipConnection(subblock), up)\n",
    "\n",
    "        self.model = _create_block(in_channels, out_channels, self.channels, self.strides, True)\n",
    "\n",
    "    def _get_down_layer(self, in_channels, out_channels, strides, is_top):\n",
    "        if self.num_res_units > 0:\n",
    "            return ResidualUnit(\n",
    "                self.dimensions,\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                strides,\n",
    "                self.kernel_size,\n",
    "                self.num_res_units,\n",
    "                self.act,\n",
    "                self.norm,\n",
    "                self.dropout,\n",
    "            )\n",
    "        else:\n",
    "            return Convolution(\n",
    "                self.dimensions, in_channels, out_channels, strides, self.kernel_size, self.act, self.norm, self.dropout\n",
    "            )\n",
    "\n",
    "    def _get_bottom_layer(self, in_channels, out_channels):\n",
    "        return self._get_down_layer(in_channels, out_channels, 1, False)\n",
    "\n",
    "    def _get_up_layer(self, in_channels, out_channels, strides, is_top):\n",
    "        conv = Convolution(\n",
    "            self.dimensions,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            strides,\n",
    "            self.up_kernel_size,\n",
    "            self.act,\n",
    "            self.norm,\n",
    "            self.dropout,\n",
    "            conv_only=is_top and self.num_res_units == 0,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "\n",
    "        if self.num_res_units > 0:\n",
    "            ru = ResidualUnit(\n",
    "                self.dimensions,\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                1,\n",
    "                self.kernel_size,\n",
    "                1,\n",
    "                self.act,\n",
    "                self.norm,\n",
    "                self.dropout,\n",
    "                last_conv_only=is_top,\n",
    "            )\n",
    "            return nn.Sequential(conv, ru)\n",
    "        else:\n",
    "            return conv\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    # MONAI UNet code ends here\n",
    "    \n",
    "    # Lightning training\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch['image'].to(device), batch_data['mask'].to(device)\n",
    "        inputs = inputs.squeeze(4)\n",
    "        labels = labels.squeeze(4)\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        tensorboard_logs = {'loss/train': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    # Lightning validation\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch['image'].to(device), batch_data['mask'].to(device)\n",
    "        inputs = inputs.squeeze(4)\n",
    "        labels = labels.squeeze(4)\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack(x['val_loss'] for x in outputs).mean()\n",
    "        tensorboard_logs = {'loss/val': avg_loss}\n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1000\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders and Transforms\n",
    "\n",
    "I'm not sure of a better way to streamline transformation testing than to just copy/paste this cell into each model's section..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformations\n",
    "data_transforms = Compose([\n",
    "    LoadNiftid(keys=data_keys),\n",
    "    ScaleIntensityd(keys=data_keys),\n",
    "    AddChanneld(keys=data_keys),\n",
    "    ToTensord(keys=data_keys)\n",
    "])\n",
    "\n",
    "train_transforms = Compose([\n",
    "    data_transforms,\n",
    "#     RandSpatialCropd(\n",
    "#         keys=data_keys,\n",
    "#         roi_size=(128, 128, 1),\n",
    "#         random_size=False\n",
    "#     ),\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    data_transforms\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    data_transforms\n",
    "])\n",
    "\n",
    "dataloaders = {\n",
    "    'train': create_loader(train_dicts, batch_size=BATCH_SIZE, transforms=train_transforms, shuffle=True),\n",
    "    'val': create_loader(val_dicts, transforms=val_transforms),\n",
    "    'test': create_loader(test_dicts, transforms=test_transforms)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = monai.losses.DiceLoss(to_onehot_y=True, do_softmax=True)\n",
    "model = UNet(\n",
    "    dimensions=2,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(64, 128, 258, 512, 1024),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    norm=monai.networks.layers.Norm.BATCH,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    criterion=criterion\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping('val_loss')\n",
    "\n",
    "trainer = Trainer(\n",
    "    check_val_every_n_epoch=10,\n",
    "    early_stop_callback=early_stopping,\n",
    "    gpus=2,\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    min_epochs=10\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloader=dataloaders['train'],\n",
    "    val_dataloader=dataloaders['val']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
