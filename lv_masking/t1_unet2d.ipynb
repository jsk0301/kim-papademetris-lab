{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "class LVDataset(Dataset):\n",
    "    # animal_nums is a list of numbers indicating which animals' images to include in the dataset\n",
    "    \n",
    "    # to try: tuple(zip(images, masks))\n",
    "    def __init__(self, animal_nums, transform=None):\n",
    "        image_folders = []\n",
    "        types = ['Baseline', 'PostGel', 'PostMI']\n",
    "        for num in animal_nums:\n",
    "            for type in types:\n",
    "                image_folders.append(datapath + 'PSEA' + str(num) + ' ' + type + '/')\n",
    "        self.image_depth = 37\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        for folder in image_folders:\n",
    "            files = np.array(os.listdir(folder))\n",
    "            images = np.sort(files[[('Mask' not in name and name != '.DS_Store') for name in files]])\n",
    "            images = [folder + image for image in images]\n",
    "            self.images.extend(images)\n",
    "            \n",
    "            masks = np.sort(files[['Mask' in name for name in files]])\n",
    "            masks = [folder + mask for mask in masks]\n",
    "            self.masks.extend(masks)\n",
    "\n",
    "        if len(self.images) != len(self.masks):\n",
    "            print('Different number of images and masks')\n",
    "\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images) * self.image_depth\n",
    "    \n",
    "    # input must be a list\n",
    "    # returns tensors that represent images/masks\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image = nib.load(self.images[idx // self.image_depth]).get_fdata()\n",
    "        image = image[:,:,idx % self.image_depth]\n",
    "        \n",
    "        mask = nib.load(self.masks[idx // self.image_depth]).get_fdata()\n",
    "        mask = mask[:,:,idx % self.image_depth]\n",
    "        sample = {'image': image, 'mask': mask}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return (sample['image'], sample['mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "- patching\n",
    "- normalization\n",
    "- slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the full U-Net without losing data,\n",
    "# we need the dimensions to be a multiple of 16\n",
    "class CropTensor(object):\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        orig_shape = list(image.shape)\n",
    "        start_ind = orig_shape[0] // 2 - self.output_size // 2\n",
    "        end_ind = orig_shape[0] // 2 + self.output_size // 2\n",
    "        \n",
    "        # Channels for cross entropy loss\n",
    "        image = image[np.newaxis, start_ind:end_ind, start_ind:end_ind]\n",
    "        mask = mask[start_ind:end_ind, start_ind:end_ind]\n",
    "        return {'image': image, 'mask': mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(datapath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_animals = [12, 13, 18, 25, 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LVDataset(all_animals[:4], transform=CropTensor(288))\n",
    "test_set = LVDataset(all_animals[5:], transform=CropTensor(288))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LVDataset([12], transform=CropTensor(288))\n",
    "test_set = LVDataset([13], transform=CropTensor(288))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of training set: ', len(train_set))\n",
    "print('Size of test set: ', len(test_set))\n",
    "print('Image Shape: ', train_set[0][0].shape)\n",
    "print('Mask Shape: ', train_set[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelled after the original UNet\n",
    "class UNet(nn.Module):\n",
    "    # This conv/relu combination results in no change in dimension for full image restoration\n",
    "    def conv_relu(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='reflect'):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                padding_mode=padding_mode\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    # This transpose doubles the dimensions\n",
    "    def conv_transpose(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1):\n",
    "        return nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            output_padding=output_padding\n",
    "        )\n",
    "    \n",
    "    def first_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            self.conv_relu(in_channels, out_channels),\n",
    "            self.conv_relu(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    # Output: (x-4)/2\n",
    "    def contract_block(self, in_channels, out_channels):\n",
    "        # Testing: adding BatchNorm2d(out_channels) after ReLU layers\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            self.conv_relu(in_channels, out_channels),\n",
    "            self.conv_relu(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def bottleneck_block(self, in_channels, mid_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            self.conv_relu(in_channels, mid_channels),\n",
    "            self.conv_relu(mid_channels, mid_channels),\n",
    "            self.conv_transpose(mid_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    # Output: (x-4)*2\n",
    "    def expand_block(self, in_channels, mid_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            self.conv_relu(in_channels, mid_channels),\n",
    "            self.conv_relu(mid_channels, mid_channels),\n",
    "            self.conv_transpose(mid_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def final_block(self, in_channels, mid_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            self.conv_relu(in_channels, mid_channels),\n",
    "            self.conv_relu(mid_channels, mid_channels),\n",
    "            nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.contraction = nn.ModuleList([\n",
    "            # 288\n",
    "            self.first_block(1, 64),\n",
    "            # 288\n",
    "            self.contract_block(64, 128),\n",
    "            # 144\n",
    "            self.contract_block(128, 256),\n",
    "            # 72\n",
    "            self.contract_block(256, 512),\n",
    "            # 36\n",
    "        ])\n",
    "        \n",
    "        self.bottleneck = self.bottleneck_block(512, 1024, 512)\n",
    "        \n",
    "        self.expansion = nn.ModuleList([\n",
    "            # 36\n",
    "            self.expand_block(1024, 512, 256),\n",
    "            # 72\n",
    "            self.expand_block(512, 256, 128),\n",
    "            # 144\n",
    "            self.expand_block(256, 128, 64),\n",
    "            # 288\n",
    "            self.final_block(128, 64, 2)\n",
    "            # 288\n",
    "        ])\n",
    "        \n",
    "        self.contraction_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        for layer in self.contraction:\n",
    "            image = layer(image)\n",
    "            self.contraction_outputs.append(image)\n",
    "        \n",
    "        image = self.bottleneck(image)\n",
    "        for i in range(4):\n",
    "            image = torch.cat((self.contraction_outputs[3 - i], image), dim=1)\n",
    "            image = self.expansion[i](image)\n",
    "        self.contraction_outputs = []\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, epochs):\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        torch.save(model.state_dict(), 'pytorch_unet.pth')\n",
    "        print('EPOCH', epoch)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            total_loss = 0\n",
    "            for batch_id, (image, mask) in enumerate(dataloaders[phase]):\n",
    "                image = image.to(device, dtype=torch.float)\n",
    "                mask = mask.to(device, dtype=torch.long)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output_mask = model(image)\n",
    "                    loss = criterion(output_mask, mask)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                del image\n",
    "                del mask\n",
    "                del output_mask\n",
    "                del loss\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            print(phase, ' loss: ', total_loss)\n",
    "            writer.add_scalar('loss/' + phase, total_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model2(model, optimizer, criterion, epochs):\n",
    "    writer = SummaryWriter()\n",
    "    VAL_INTERVAL = 5\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "\n",
    "    best_metric_epoch = -1\n",
    "\n",
    "    phase = 'val'\n",
    "    epoch = 0\n",
    "    while epoch < NUM_EPOCHS:\n",
    "        if epoch % VAL_INTERVAL == 0 and phase == 'train':\n",
    "            epoch -= 1\n",
    "            phase = 'val'\n",
    "            print('-' * 10)\n",
    "            print('Validation')\n",
    "\n",
    "        else:\n",
    "            phase = 'train'\n",
    "            print('-' * 10)\n",
    "            print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n",
    "            lr = get_lr(optimizer)\n",
    "            print('Learning Rate: ', lr)\n",
    "            writer.add_scalar('learning_rate', lr, epoch + 1)\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in data_loaders[phase]:\n",
    "            step += 1\n",
    "            inputs, labels = batch_data['image'].to(device), batch_data['mask'].to(device)\n",
    "            inputs = inputs.squeeze(4)\n",
    "            labels = labels.squeeze(4)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= step\n",
    "\n",
    "        if phase == 'train':\n",
    "            epoch_loss_values.append(epoch_loss)\n",
    "            print('epoch {} average loss: {:.4f}'.format(epoch + 1, epoch_loss))\n",
    "\n",
    "        else:\n",
    "            metric_values.append(epoch_loss)\n",
    "            best_metric = min(metric_values)\n",
    "            if epoch_loss <= best_metric:\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), 'best_metric_model.pth')\n",
    "                print('Saved new best metric model')\n",
    "            print('val average loss: {:.4f}'.format(epoch_loss))\n",
    "            print('best val loss: {:.4f} at epoch {}'.format(best_metric, best_metric_epoch))\n",
    "\n",
    "        writer.add_scalar('loss/' + phase, epoch_loss, epoch + 1)\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, optimizer, criterion, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/6-3-2020_full_pytorch_unet/best_metric_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_id, (image, mask) in enumerate(dataloaders['val']):\n",
    "        image = image.to(device, dtype=torch.float)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        if i > 5:\n",
    "            break\n",
    "        \n",
    "        roi_size = (64, 64, 1)\n",
    "        sw_batch_size = 4\n",
    "#         output = sliding_window_inference(\n",
    "#             test_data['image'].to(device),\n",
    "#             roi_size,\n",
    "#             sw_batch_size,\n",
    "#             model\n",
    "#         ),\n",
    "        output = model(image)\n",
    "        print(output.shape)\n",
    "        plt.figure('check')\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Image ' + str(i))\n",
    "        plt.imshow(image[0][0], cmap='gray')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Mask ' + str(i))\n",
    "        plt.imshow(mask)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Output ' + str(i))\n",
    "        output = torch.argmax(output, dim=1).detach().cpu()[0, :, :]\n",
    "        print(output.shape)\n",
    "        plt.imshow(output)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
